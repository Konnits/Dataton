{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbdeaac-dc7c-4d05-898c-0e681980b146",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe766444-4747-4369-9cc8-391454da4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90f9aaf-9660-4089-b245-5ca371c51854",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e746715-a8cf-468c-b4f7-a12ef5ce19b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entrenamiento normal con algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8b75e-57a0-4b21-8bef-dc80c6329acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genetic_model:\n",
    "    \n",
    "    def __init__(self, nn_model = \"\", nlp_model = \"\", params = \"\", vector_size = 100,population = 50, min_count = 2):\n",
    "        \n",
    "        self.nn_model = nn_model\n",
    "        self.nlp_model = nlp_model\n",
    "        self.parameters = params\n",
    "        self.vector_size = vector_size\n",
    "        self.population = population\n",
    "        self.min_count = min_count\n",
    "        self.optimizer = tf.keras.optimizers.Adamax(learning_rate = 1e-5)\n",
    "        self.metrics_binary = [self.f1_macro]\n",
    "        self.indexes = [\"is_hate\", \"no_hate\"]\n",
    "        \n",
    "    def f1_macro(self, y_true, y_pred):\n",
    "        \n",
    "        def recall_m(y_true, y_pred):\n",
    "            \n",
    "            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            recall = TP / (Positives+K.epsilon())\n",
    "            return recall\n",
    "    \n",
    "        def precision_m(y_true, y_pred):\n",
    "            \n",
    "            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            #loss = tf.cast(loss, tf.float32)\n",
    "            precision = tf.cast(TP, tf.float32) / tf.cast(Pred_Positives, tf.float32)+K.epsilon()\n",
    "                \n",
    "            return precision\n",
    "    \n",
    "        precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "        return -2*tf.cast((tf.cast(precision, tf.double)*tf.cast(recall, tf.double))/tf.cast(tf.cast(precision, tf.float32)+tf.cast(recall, tf.float32)+K.epsilon(), tf.double), tf.float32)\n",
    "\n",
    "    def get_params(self):\n",
    "        file = open(self.parameters)\n",
    "        params = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        mutation_prob = params[\"mutation_prob\"]\n",
    "        mutation_importance = params[\"mutation_importance\"]\n",
    "        \n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.mutation_importance = mutation_importance\n",
    "        \n",
    "    def pair_two_nn(self, nn_1, nn_2):\n",
    "        \n",
    "        self.get_params()\n",
    "\n",
    "        nn_s = tf.keras.models.clone_model(nn_1)\n",
    "\n",
    "        layers = [(nn_1.layers[i], nn_2.layers[i]) for i in range(1, len(nn_1.layers))]\n",
    "        importance = np.random.uniform()\n",
    "\n",
    "        l = 1\n",
    "        for l1, l2 in layers:\n",
    "            if type(l1) is tf.keras.layers.Dense:\n",
    "                trainable = l1.get_weights()[1]\n",
    "\n",
    "                w1 = l1.get_weights()[0]\n",
    "                w2 = l2.get_weights()[0]\n",
    "                w = importance * w1 + (1 - importance) * w2\n",
    "\n",
    "                if np.random.uniform() < self.mutation_prob:\n",
    "                    shape = nn_1.layers[l].get_weights()[0].shape\n",
    "                    w += np.random.normal(size = shape) * self.mutation_importance\n",
    "                    w /= 1 + self.mutation_importance\n",
    "\n",
    "                nn_s.layers[l].set_weights([w, trainable])\n",
    "            l += 1\n",
    "\n",
    "        nn_s.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return nn_s\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ü', 'u')\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip(\" \")\n",
    "        return text\n",
    "    \n",
    "    def train_nlp_model(self, list_of_files = [], save_model = False, model_name = \"\"):\n",
    "        \n",
    "        data = list(map( lambda x : pd.read_csv(x).rename({\"Text\" : \"text\"}, axis = 1)[\"text\"].values.tolist() , list_of_files ))\n",
    "        \n",
    "        texts = []\n",
    "        for i in data:\n",
    "            texts += i\n",
    "        \n",
    "        data = pd.DataFrame({\"text\" : texts})\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x : self.preprocess_text(x))\n",
    "        data[\"all_words\"] = data[\"text\"].apply(lambda x : self.tokenize(x))\n",
    "        word2vec = Word2Vec(data[\"all_words\"].values.tolist(), min_count = self.min_count, vector_size = self.vector_size)\n",
    "        \n",
    "        self.word2vec = word2vec\n",
    "        self.vocab = set(list(word2vec.wv.index_to_key))\n",
    "        \n",
    "        if save_model:\n",
    "            word2vec.save(model_name)\n",
    "        \n",
    "    def tokenize(self, x): #------------------------------- MEJORAR LA TOKENIZACION --------------------------\n",
    "        \n",
    "        return x.strip(\" \").split(\" \")\n",
    "    \n",
    "    def preprocess_dataset(self, dataset, batch_size, size):\n",
    "        \n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        dataset = dataset.shuffle(size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        return dataset\n",
    "    \n",
    "    def make_nns(self):\n",
    "        self.neural_networks = [self.create_nn() for i in range(self.population)]\n",
    "    \n",
    "    def train_hate(self, device = \"/GPU:0\", generations = 100, train = True):\n",
    "        \n",
    "        self.get_params()\n",
    "        counter = 0\n",
    "        \n",
    "        with tf.device(device):\n",
    "            \n",
    "            self.best_hate_score = 1\n",
    "            actual_best = 0\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor = \"val_f1_macro\", patience = 3)]\n",
    "            while counter < generations:\n",
    "\n",
    "                X_train_fold, X_val_fold, y_train_fold, y_val_fold = self.k_fold_sets[counter % 10]\n",
    "\n",
    "                X_train_fold = X_train_fold.copy()\n",
    "                X_val_fold = X_val_fold.copy()\n",
    "                \n",
    "                if train:\n",
    "                    data_train_tf = tf.data.Dataset.from_tensor_slices((X_train_fold[\"text\"].values.tolist(), y_train_fold[self.indexes].values.tolist()))\n",
    "                    data_val_tf = tf.data.Dataset.from_tensor_slices((X_val_fold[\"text\"].values.tolist(), y_val_fold[self.indexes].values.tolist()))\n",
    "                    \n",
    "                    cache_train = self.preprocess_dataset(data_train_tf, 16, len(data_train_tf))\n",
    "                    cache_val = self.preprocess_dataset(data_val_tf, 16, len(data_val_tf))\n",
    "                    \n",
    "                n = 1\n",
    "                if not train:\n",
    "                    clear_output(wait = True)\n",
    "                    print(f\"Generation : {counter} - prob : {self.mutation_prob}, importance : {self.mutation_importance}\\nBest score : {self.best_hate_score}\\nActual best : {actual_best}\")\n",
    "                self.scores = []\n",
    "                if train:\n",
    "                    for neural_network in self.neural_networks:\n",
    "                        clear_output(wait = True)\n",
    "                        print(f\"Generation : {counter} - prob : {self.mutation_prob}, importance : {self.mutation_importance}\\nBest score : {self.best_hate_score}\\nActual best : {actual_best}\")\n",
    "                        print(f\"Training neural network : {n}\")\n",
    "                        try:\n",
    "                            neural_network.fit(cache_train, validation_data = cache_val, callbacks = callbacks, epochs = 32)\n",
    "                        except:\n",
    "                            neural_network.compile(\n",
    "                                optimizer = self.optimizer,\n",
    "                                loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "                                metrics = self.metrics_binary\n",
    "                            )\n",
    "                            neural_network.fit(cache_train, validation_data = cache_val, callbacks = callbacks, epochs = 32)\n",
    "#                         try:\n",
    "#                             score = self.f1_macro(y_val_fold[self.indexes].values.reshape((len(y_val_fold),2)), neural_network.predict(tf.convert_to_tensor(X_val_fold[\"text\"].values.tolist())))\n",
    "                        \n",
    "#                         except:\n",
    "#                             neural_network.compile(\n",
    "#                                 optimizer = self.optimizer,\n",
    "#                                 loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "#                                 metrics = self.metrics_binary\n",
    "#                             )\n",
    "                        score = self.f1_macro(tf.cast(y_val_fold[self.indexes].values.tolist(), tf.float32), tf.cast(neural_network.predict(tf.convert_to_tensor(X_val_fold[\"text\"].values.tolist())), tf.float32))\n",
    "                        self.scores.append((score, n-1))\n",
    "                        n += 1\n",
    "                else:\n",
    "                    y = y_train_fold[self.indexes].values.tolist()\n",
    "                    y = np.array(y).reshape((len(y), 2))\n",
    "                    # score = f1_score(y, neural_network.predict(tf.convert_to_tensor(X_train_fold[\"text\"].values.tolist())))\n",
    "                    v = list(map(lambda x : x.tolist(), X_train_fold[\"text\"].values.tolist()))\n",
    "                    v = tf.convert_to_tensor(v)\n",
    "                    scores = list(map(lambda x : self.f1_macro(tf.cast(y, tf.float32), tf.cast(x.predict(v), tf.float32)) ,self.neural_networks))\n",
    "                    self.scores = [(scr, pos) for pos, scr in enumerate(scores)]\n",
    "                    \n",
    "                self.scores.sort()\n",
    "                self.scores.reverse()\n",
    "\n",
    "                best_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-1][1]])\n",
    "                second_best_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-2][1]])\n",
    "                third_bes_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-3][1]])\n",
    "\n",
    "                if self.best_hate_score > self.scores[-1][0]:\n",
    "                    self.best_hate_score = self.scores[-1][0]\n",
    "                    self.best_hate_model = tf.keras.models.clone_model(best_model)\n",
    "\n",
    "\n",
    "                actual_best = self.scores[-1][0]\n",
    "                aux = [best_model]\n",
    "\n",
    "                for i in range(self.population // 2):\n",
    "                    pos = np.random.choice(range(self.population))\n",
    "                    aux.append(self.pair_two_nn(best_model, self.neural_networks[pos]))\n",
    "                for i in range(self.population // 2-1):\n",
    "                    pos = np.random.choice(range(self.population))\n",
    "                    aux.append(self.pair_two_nn(second_best_model, self.neural_networks[pos]))\n",
    "                \n",
    "                self.neural_networks.clear()\n",
    "                self.neural_networks = aux.copy()\n",
    "                del aux\n",
    "\n",
    "                counter += 1\n",
    "                \n",
    "    def k_fold(self, X, y, k):\n",
    "        \n",
    "        X[\"text\"] = X[\"text\"].apply(lambda x : self.phrase_to_vect(x))\n",
    "        \n",
    "        self.k_fold_sets = []\n",
    "        for i in range(k):\n",
    "            self.k_fold_sets.append( tuple(train_test_split(X, y, test_size = 1 / k)) )\n",
    "            \n",
    "    def apply(self, d, t, i):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            return np.sin(t * 10000**(i / d))\n",
    "        return np.cos(t * 10000**((i - 1) / d))\n",
    "\n",
    "    def positional_encoding(self, d, t):\n",
    "        \n",
    "        vector = list(range(0, d))\n",
    "        vector = list(map(lambda x : self.apply(d, t, x), vector))\n",
    "        return np.array(vector)\n",
    "\n",
    "    def phrase_to_vect(self, x):\n",
    "        \n",
    "        phrases = self.tokenize(x)\n",
    "        phrases = [i for i in phrases if i in self.vocab]\n",
    "        \n",
    "        return sum(list(map(lambda x : self.word2vec.wv.get_vector(x) * self.positional_encoding(self.vector_size, phrases.index(x)), phrases))).astype(np.float32)\n",
    "    \n",
    "    def load_word2vec_model(self):\n",
    "        self.word2vec = Word2Vec.load(self.nlp_model)\n",
    "        self.vocab = set(list(self.word2vec.wv.index_to_key))\n",
    "    \n",
    "    def load_nnmodel(self):\n",
    "        self.model_hate = tf.keras.models.load_model(self.nn_model)\n",
    "        \n",
    "    def create_nn(self):\n",
    "        input_layer = tf.keras.Input(shape = (self.vector_size, 1))\n",
    "        layer_1 = tf.keras.layers.Dense(self.vector_size, activation = \"relu\")(input_layer)\n",
    "        layer_2 = tf.keras.layers.Dense(100, activation = \"relu\")(layer_1)\n",
    "        output = tf.keras.layers.Dense(2, activation = \"softmax\")(layer_2)\n",
    "\n",
    "        model = tf.keras.Model(inputs = input_layer, outputs = output)\n",
    "        model.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def attention(self): # ---------------------- TODO ----------------------------\n",
    "        pass\n",
    "    \n",
    "    def set_best_hate_model(self):\n",
    "        \n",
    "        self.model_hate = tf.keras.models.clone_model(self.best_hate_model)\n",
    "        self.model_hate.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "    \n",
    "    def predict_hate(self, X):\n",
    "        \n",
    "        X[\"text\"] = X[\"text\"].apply(lambda x : self.phrase_to_vect(self.preprocess_text(x)))\n",
    "        return self.model_hate.predict(tf.convert_to_tensor(X[\"text\"].values.tolist()))\n",
    "    \n",
    "    def get_new_phrase(self, text):\n",
    "\n",
    "        word = np.random.choice(self.tokenize(text))\n",
    "        while word not in self.word2vec.wv.index_to_key:\n",
    "            word = word = np.random.choice(self.tokenize(text))\n",
    "        new_word = self.word2vec.wv.most_similar(word)[np.random.choice(range(5))][0]\n",
    "        return text.replace(word, new_word)\n",
    "    \n",
    "    def create_new_data(self, data, size):\n",
    "        \n",
    "        return list(map(lambda x : self.get_new_phrase(np.random.choice(data)), range(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2015bd-93b9-45ff-9fb8-bf9e9470a53e",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo de Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0978e2-5d0e-4e6d-b4f8-3f530e474c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Dataton\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20bb259-dab2-4d28-bcbe-e308f53a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    if \"data_found\" in file:\n",
    "        texts.append(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Dataton\\\\\" + file)\n",
    "        \n",
    "texts.append(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Dataton\\\\Datasets\\\\Spanish Toxicity Dataset.csv\")\n",
    "texts.append(\"public_test_data.csv\")\n",
    "texts.append(\"referenced_tweets_data.csv\")\n",
    "texts.append(\"tweets_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabc4a24-f588-456d-831e-8ac4f70cc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = Genetic_model(params = \"./Parameters/parameters.json\", nlp_model = \"./Model/model\", population = 100, vector_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d7958-3ca3-4436-ac55-77716ad04fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.train_nlp_model(texts, True, \"./Model/model\")\n",
    "# Trainer.load_word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b324e-0602-4e75-9d58-79c8a1a37a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.word2vec.wv.most_similar(\"hueona\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f60d0d-1cfe-4756-a957-61b77784667d",
   "metadata": {},
   "source": [
    "## Creamos la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ab80c-6f31-4865-9959-f823d6562756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2b0c1-9d38-44e5-975d-17afacd4f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets_train.csv\")\n",
    "referenced = pd.read_csv(\"referenced_tweets_data.csv\")[[\"tweet_id\", \"conversation_id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc58ebf-8a95-4b94-8b50-df752c088dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x : Trainer.preprocess_text(x))\n",
    "referenced[\"text\"] = referenced[\"text\"].apply(lambda x : Trainer.preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ae523-6d2f-4d71-8358-189e93fb934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {}\n",
    "for l in referenced.values.tolist():\n",
    "    tweet_id = l[0]\n",
    "    conv_id = l[1]\n",
    "    text_string = l[2]\n",
    "    \n",
    "    if conv_id not in transform and conv_id != 0:\n",
    "        transform[conv_id] = []\n",
    "    if conv_id != 0:\n",
    "        transform[conv_id].append((tweet_id, text_string.strip(\" \")))\n",
    "        transform[conv_id].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299b635-9bd5-4ac0-99ef-a634ffa8c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Odio\"] = data[\"Odio\"].apply(lambda x : 1 - int(x == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd009ca-a68a-471e-9b79-a9eb092d4cb9",
   "metadata": {},
   "source": [
    "### Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef0e39-c45f-4aee-9a9a-0f01bd0b9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_new_data = 5000\n",
    "\n",
    "hate_data = data.query(\"Odio == 1\")[\"text\"]\n",
    "not_hate_data = data.query(\"Odio == 0\")[\"text\"]\n",
    "\n",
    "new_hate_data = Trainer.create_new_data(hate_data.values.tolist(), number_of_new_data)\n",
    "new_nohate_data = Trainer.create_new_data(not_hate_data.values.tolist(), number_of_new_data)\n",
    "\n",
    "text_column = data[\"text\"].values.tolist() + new_hate_data + new_nohate_data\n",
    "hate_column = data[\"Odio\"].values.tolist() + [ 1 for i in range(len(new_hate_data))] + [ 1 for i in range(len(new_nohate_data))]\n",
    "\n",
    "full_data = pd.DataFrame({\"text\" : text_column, \"Odio\" : hate_column})\n",
    "\n",
    "full_data[\"is_hate\"] = full_data[\"Odio\"].apply(lambda x : int(x == 1))\n",
    "full_data[\"no_hate\"] = full_data[\"Odio\"].apply(lambda x : int(x == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65713956-5c2e-4dff-980c-7b6ec3c07a4c",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7d945-d0de-4c11-a91e-baa9d351bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data[[\"text\"]], full_data[[\"is_hate\", \"no_hate\"]], test_size = 0.1)\n",
    "y_train = y_train.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f724ed-c423-41c2-9eff-fa44c3955324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.k_fold(X_train.copy(), y_train.copy(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6109c6b-3626-43ba-a30e-af2eab7a2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.make_nns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8c252-c9b2-4939-9530-1ab83c5330d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.neural_networks[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d75d6-b595-45c8-9015-ac443eef56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Trainer.train_hate(generations = 5, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4570ea5-b4f0-4ce1-9471-b5d511003c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.neural_networks[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b0726-790d-46ca-8915-dd3e8ebab860",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.set_best_hate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c9fe6-bb9f-4223-baa1-3b581afe0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.predict_hate(pd.DataFrame({\n",
    "    \"text\" : [\"\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883cd12-0a65-4eed-819e-a2e639a72c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc820b5a-8834-418d-a21a-214381814143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Trainer.neural_networks[0]\n",
    "Trainer.model_hate = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96760e1-c433-44ca-a643-56d28f3cace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"text\" : [\"yo no creeria en lo que dice\"]})\n",
    "Trainer.f1_macro(tf.cast([[1,0]], tf.float32), tf.cast(Trainer.predict_hate(test).round().tolist(), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de06777-f15f-40ec-b1d5-89b96e6ad2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genetic_model_v2(Genetic_model):\n",
    "    def __init__(self, nn_model = \"\", nlp_model = \"\", params = \"\", vector_size = 100,population = 50, min_count = 2):\n",
    "        self.nn_model = nn_model\n",
    "        self.nlp_model = nlp_model\n",
    "        self.parameters = params\n",
    "        self.vector_size = vector_size\n",
    "        self.population = population\n",
    "        self.min_count = min_count\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        self.metrics_binary = [self.f1_macro]\n",
    "        \n",
    "    def create_nn(self):\n",
    "        input_layer = tf.keras.Input(shape = self.vector_size)\n",
    "        layer_1 = tf.keras.layers.Dense(50, activation = \"relu\")(input_layer)\n",
    "        layer_2 = tf.keras.layers.Dense(25, activation = \"relu\")(layer_1)\n",
    "        output = tf.keras.layers.Dense(1, activation = \"sigmoid\")(layer_1)\n",
    "\n",
    "        model = tf.keras.Model(inputs = input_layer, outputs = output)\n",
    "        model.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c85e6-2ebb-4765-8ff2-4aad8c156ec3",
   "metadata": {},
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9f782-40d6-4979-b3fb-ca92d9244d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b6190-1037-4397-baa1-e5f7ae90f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbff48-814d-4223-b124-6fd0b4707b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tf.keras.Model(text_input, pooled_output)\n",
    "sentences = tf.constant([\"que pasa lacra qla\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea00c54-d465-440f-b9d0-1d40a63d9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_model)\n",
    "model.add(tf.keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76770552-197f-497e-8ebc-b5af5c6d3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = tf.constant(X_train[\"text\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6be934-a96e-4b6b-9ef6-4ad57264aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[\"is_hate\"] = y_train[\"Odio\"].apply(lambda x : int(x == 1))\n",
    "y_train[\"not_hate\"] = y_train[\"Odio\"].apply(lambda x : int(x == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20e193-6aa9-4a4f-a439-0cbcc527af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = tf.constant(y_train[[\"is_hate\", \"not_hate\"]].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7257c3-def9-4991-9c5b-e80e08759ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "    metrics = tf.keras.losses.BinaryCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b546c1-365e-4221-b53b-50dda5e86a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(\"/GPU:0\"):\n",
    "#     model.fit(x_train_tensor, y_train_tensor, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4192c6-8a9b-4b56-a46a-d5abd2c8190f",
   "metadata": {},
   "source": [
    "# Reuse a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95a201-cf9d-4cca-a421-d52dc01443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162686dc-4cab-401c-a3c7-365394bfbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"sigmoid\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3c913-13a3-4845-b177-59def4b69677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc06886-bf7c-4079-8c36-9e763f08c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_tensor, y_train_tensor, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820e125-68f7-4994-a383-b7d8c8267957",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(tf.constant([\"\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbdeaac-dc7c-4d05-898c-0e681980b146",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe766444-4747-4369-9cc8-391454da4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90f9aaf-9660-4089-b245-5ca371c51854",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e746715-a8cf-468c-b4f7-a12ef5ce19b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entrenamiento normal con algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15d8b75e-57a0-4b21-8bef-dc80c6329acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genetic_model:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nn_model = \"\",\n",
    "                 nlp_model = \"\",\n",
    "                 params = \"\",\n",
    "                 vector_size = 100,\n",
    "                 population = 50,\n",
    "                 min_count = 2,\n",
    "                 embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"):\n",
    "        \n",
    "        self.nn_model = nn_model\n",
    "        self.nlp_model = nlp_model\n",
    "        self.parameters = params\n",
    "        self.vector_size = vector_size\n",
    "        self.population = population\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n",
    "        self.metrics_binary = [tf.keras.metrics.categorical_crossentropy]\n",
    "        self.loss = tf.keras.losses.categorical_crossentropy\n",
    "        \n",
    "        self.indexes = [\"Odio\", \"Mujeres\", \"Comunidad LGBTQ+\", \"Comunidades Migrantes\", \"Pueblos Originarios\"]\n",
    "        self.hub_layer = hub.KerasLayer(embedding, input_shape = [], dtype = tf.string, trainable = False, name = \"Embedding\")\n",
    "        \n",
    "    def f1_macro(self, y_true, y_pred):\n",
    "        \n",
    "        def recall_m(y_true, y_pred):\n",
    "            \n",
    "            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            recall = TP / (Positives+K.epsilon())\n",
    "            return recall\n",
    "    \n",
    "        def precision_m(y_true, y_pred):\n",
    "            \n",
    "            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            #loss = tf.cast(loss, tf.float32)\n",
    "            precision = tf.cast(TP, tf.float32) / tf.cast(Pred_Positives, tf.float32)+K.epsilon()\n",
    "                \n",
    "            return precision\n",
    "    \n",
    "        precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "        return -2*tf.cast((tf.cast(precision, tf.double)*tf.cast(recall, tf.double))/tf.cast(tf.cast(precision, tf.float32)+tf.cast(recall, tf.float32)+K.epsilon(), tf.double), tf.float32)\n",
    "\n",
    "    def get_params(self):\n",
    "        file = open(self.parameters)\n",
    "        params = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        mutation_prob = params[\"mutation_prob\"]\n",
    "        mutation_importance = params[\"mutation_importance\"]\n",
    "        \n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.mutation_importance = mutation_importance\n",
    "        \n",
    "    def pair_two_nn(self, nn_1, nn_2):\n",
    "        \n",
    "        self.get_params()\n",
    "\n",
    "        nn_s = tf.keras.models.clone_model(nn_1)\n",
    "\n",
    "        layers = [(nn_1.layers[i], nn_2.layers[i]) for i in range(1, len(nn_1.layers))]\n",
    "        importance = np.random.uniform()\n",
    "\n",
    "        l = 1\n",
    "        for l1, l2 in layers:\n",
    "            if type(l1) is tf.keras.layers.Dense:\n",
    "                bias = l1.get_weights()[1]\n",
    "\n",
    "                w1 = l1.get_weights()[0]\n",
    "                w2 = l2.get_weights()[0]\n",
    "                b2 = l2.get_weights()[1]\n",
    "                \n",
    "                w = importance * w1 + (1 - importance) * w2\n",
    "                bias = importance * bias + (1 - importance) * b2\n",
    "\n",
    "                if np.random.uniform() < self.mutation_prob:\n",
    "                    shape = nn_1.layers[l].get_weights()[0].shape\n",
    "                    shape_bias = bias.shape\n",
    "                    w += np.random.normal(size = shape) * self.mutation_importance\n",
    "                    w /= 1 + self.mutation_importance\n",
    "                    b += np.random.normal(size = shape_bias) * self.mutation_importance\n",
    "                    bias /= 1 + self.mutation_importance\n",
    "\n",
    "                nn_s.layers[l].set_weights([w, bias])\n",
    "            l += 1\n",
    "\n",
    "        nn_s.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return nn_s\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('ü', 'u')\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip(\" \")\n",
    "        return text\n",
    "    \n",
    "    def train_nlp_model(self, list_of_files = [], save_model = False, model_name = \"\"):\n",
    "        \n",
    "        data = list(map( lambda x : pd.read_csv(x).rename({\"Text\" : \"text\"}, axis = 1)[\"text\"].values.tolist() , list_of_files ))\n",
    "        \n",
    "        texts = []\n",
    "        for i in data:\n",
    "            texts += i\n",
    "        \n",
    "        data = pd.DataFrame({\"text\" : texts})\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x : self.preprocess_text(x))\n",
    "        data[\"all_words\"] = data[\"text\"].apply(lambda x : self.tokenize(x))\n",
    "        word2vec = Word2Vec(data[\"all_words\"].values.tolist(), min_count = self.min_count, vector_size = self.vector_size)\n",
    "        \n",
    "        self.word2vec = word2vec\n",
    "        self.vocab = set(list(word2vec.wv.index_to_key))\n",
    "        \n",
    "        if save_model:\n",
    "            word2vec.save(model_name)\n",
    "        \n",
    "    def tokenize(self, x): #------------------------------- MEJORAR LA TOKENIZACION --------------------------\n",
    "        \n",
    "        return x.strip(\" \").split(\" \")\n",
    "    \n",
    "    def preprocess_dataset(self, dataset, batch_size, size):\n",
    "        \n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        dataset = dataset.shuffle(size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        return dataset\n",
    "    \n",
    "    def make_nns(self):\n",
    "        self.neural_networks = [self.create_nn() for i in range(self.population)]\n",
    "    \n",
    "    def train_hate(self, device = \"/GPU:0\", generations = 100, train = True):\n",
    "        \n",
    "        self.get_params()\n",
    "        counter = 0\n",
    "        \n",
    "        with tf.device(device):\n",
    "            \n",
    "            self.best_hate_score = 1\n",
    "            actual_best = 0\n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 3)]\n",
    "            while counter < generations:\n",
    "\n",
    "                X_train_fold, X_val_fold, y_train_fold, y_val_fold = self.k_fold_sets[counter % 10]\n",
    "\n",
    "                X_train_fold = X_train_fold.copy()\n",
    "                X_val_fold = X_val_fold.copy()\n",
    "                \n",
    "                if train:\n",
    "                    data_train_tf = tf.data.Dataset.from_tensor_slices((X_train_fold[\"text\"].values.tolist(), y_train_fold[self.indexes].values.tolist()))\n",
    "                    data_val_tf = tf.data.Dataset.from_tensor_slices((X_val_fold[\"text\"].values.tolist(), y_val_fold[self.indexes].values.tolist()))\n",
    "                    \n",
    "                    cache_train = self.preprocess_dataset(data_train_tf, 16, len(data_train_tf))\n",
    "                    cache_val = self.preprocess_dataset(data_val_tf, 16, len(data_val_tf))\n",
    "                    \n",
    "                n = 1\n",
    "                if not train:\n",
    "                    clear_output(wait = True)\n",
    "                    print(f\"Generation : {counter} - prob : {self.mutation_prob}, importance : {self.mutation_importance}\\nBest score : {self.best_hate_score}\\nActual best : {actual_best}\")\n",
    "                self.scores = []\n",
    "                if train:\n",
    "                    for neural_network in self.neural_networks:\n",
    "                        clear_output(wait = True)\n",
    "                        print(f\"Generation : {counter} - prob : {self.mutation_prob}, importance : {self.mutation_importance}\\nBest score : {self.best_hate_score}\\nActual best : {actual_best}\")\n",
    "                        print(f\"Training neural network : {n}\")\n",
    "                        try:\n",
    "                            neural_network.fit(cache_train, validation_data = cache_val, callbacks = callbacks, epochs = 32)\n",
    "                        except:\n",
    "                            neural_network.compile(\n",
    "                                optimizer = self.optimizer,\n",
    "                                loss = self.loss ,\n",
    "                                metrics = self.metrics_binary\n",
    "                            )\n",
    "                            neural_network.fit(cache_train, validation_data = cache_val, callbacks = callbacks, epochs = 32)\n",
    "#                         try:\n",
    "#                             score = self.f1_macro(y_val_fold[self.indexes].values.reshape((len(y_val_fold),2)), neural_network.predict(tf.convert_to_tensor(X_val_fold[\"text\"].values.tolist())))\n",
    "                        \n",
    "#                         except:\n",
    "#                             neural_network.compile(\n",
    "#                                 optimizer = self.optimizer,\n",
    "#                                 loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "#                                 metrics = self.metrics_binary\n",
    "#                             )\n",
    "                        score = self.f1_macro(tf.cast(y_val_fold[self.indexes].values.tolist(), tf.float32), tf.cast(neural_network.predict(tf.convert_to_tensor(X_val_fold[\"text\"].values.tolist())), tf.float32))\n",
    "                        self.scores.append((score, n-1))\n",
    "                        n += 1\n",
    "                else:\n",
    "                    y = y_train_fold[self.indexes].values.tolist()\n",
    "                    y = np.array(y).reshape((len(y), 2))\n",
    "                    # score = f1_score(y, neural_network.predict(tf.convert_to_tensor(X_train_fold[\"text\"].values.tolist())))\n",
    "                    v = list(map(lambda x : x.tolist(), X_train_fold[\"text\"].values.tolist()))\n",
    "                    v = tf.convert_to_tensor(v)\n",
    "                    scores = list(map(lambda x : self.f1_macro(tf.cast(y, tf.float32), tf.cast(x.predict(v), tf.float32)) ,self.neural_networks))\n",
    "                    self.scores = [(scr, pos) for pos, scr in enumerate(scores)]\n",
    "                    \n",
    "                self.scores.sort()\n",
    "                self.scores.reverse()\n",
    "\n",
    "                best_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-1][1]])\n",
    "                second_best_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-2][1]])\n",
    "                third_bes_model = tf.keras.models.clone_model(self.neural_networks[self.scores[-3][1]])\n",
    "\n",
    "                if self.best_hate_score > self.scores[-1][0]:\n",
    "                    self.best_hate_score = self.scores[-1][0]\n",
    "                    self.best_hate_model = tf.keras.models.clone_model(best_model)\n",
    "\n",
    "\n",
    "                actual_best = self.scores[-1][0]\n",
    "                aux = [best_model]\n",
    "\n",
    "                for i in range(self.population // 2):\n",
    "                    pos = np.random.choice(range(self.population))\n",
    "                    aux.append(self.pair_two_nn(best_model, self.neural_networks[pos]))\n",
    "                for i in range(self.population // 2-1):\n",
    "                    pos = np.random.choice(range(self.population))\n",
    "                    aux.append(self.pair_two_nn(second_best_model, self.neural_networks[pos]))\n",
    "                \n",
    "                self.neural_networks.clear()\n",
    "                self.neural_networks = aux.copy()\n",
    "                del aux\n",
    "\n",
    "                counter += 1\n",
    "                \n",
    "    def k_fold(self, X, y, k, size):\n",
    "        \n",
    "        self.k_fold_sets = []\n",
    "        for i in range(k):\n",
    "            self.k_fold_sets.append( tuple(train_test_split(X, y, test_size = size)) )\n",
    "            \n",
    "    def apply(self, d, t, i):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            return np.sin(t * 10000**(i / d))\n",
    "        return np.cos(t * 10000**((i - 1) / d))\n",
    "\n",
    "    def positional_encoding(self, d, t):\n",
    "        \n",
    "        vector = list(range(0, d))\n",
    "        vector = list(map(lambda x : self.apply(d, t, x), vector))\n",
    "        return np.array(vector)\n",
    "\n",
    "    def phrase_to_vect(self, x):\n",
    "        \n",
    "        phrases = self.tokenize(x)\n",
    "        phrases = [i for i in phrases if i in self.vocab]\n",
    "        \n",
    "        return sum(list(map(lambda x : self.word2vec.wv.get_vector(x) * self.positional_encoding(self.vector_size, phrases.index(x)), phrases))).astype(np.float32)\n",
    "    \n",
    "    def load_word2vec_model(self):\n",
    "        self.word2vec = Word2Vec.load(self.nlp_model)\n",
    "        self.vocab = set(list(self.word2vec.wv.index_to_key))\n",
    "    \n",
    "    def load_nnmodel(self):\n",
    "        self.model_hate = tf.keras.models.load_model(self.nn_model)\n",
    "        \n",
    "    def create_nn(self):\n",
    "        text_input = tf.keras.Input(shape=(), dtype = tf.string, name = \"Input\")\n",
    "        input_layer = self.hub_layer(text_input)\n",
    "\n",
    "        layer_1 = tf.keras.layers.Dense(16, activation = \"relu\", name = \"Layer_1\", use_bias = True)(input_layer)\n",
    "        layer_2 = tf.keras.layers.Dense(10, activation = \"relu\", name = \"Layer_2\", use_bias = True)(layer_1)\n",
    "        out_1 = tf.keras.layers.Dense(1, activation = \"sigmoid\", name = \"Out_1\", use_bias = True)(layer_2)\n",
    "        out_2 = tf.keras.layers.Dense(4, activation = \"sigmoid\", name = \"Out_2\", use_bias = True)(tf.concat([out_1, layer_2], 1))\n",
    "\n",
    "        model = tf.keras.Model(inputs = text_input, outputs = tf.concat([out_1, out_2], 1))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = self.loss ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def attention(self): # ---------------------- TODO ----------------------------\n",
    "        pass\n",
    "    \n",
    "    def set_best_hate_model(self):\n",
    "        \n",
    "        self.model_hate = tf.keras.models.clone_model(self.best_hate_model)\n",
    "        self.model_hate.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "    \n",
    "    def predict_hate(self, X):\n",
    "        \n",
    "        X[\"text\"] = X[\"text\"].apply(lambda x : self.phrase_to_vect(self.preprocess_text(x)))\n",
    "        return self.model_hate.predict(tf.convert_to_tensor(X[\"text\"].values.tolist()))\n",
    "    \n",
    "    def get_new_phrase(self, text):\n",
    "\n",
    "        word = np.random.choice(self.tokenize(text))\n",
    "        while word not in self.word2vec.wv.index_to_key:\n",
    "            word = word = np.random.choice(self.tokenize(text))\n",
    "        new_word = self.word2vec.wv.most_similar(word)[np.random.choice(range(5))][0]\n",
    "        return text.replace(word, new_word)\n",
    "    \n",
    "    def create_new_data(self, data, size):\n",
    "        \n",
    "        return list(map(lambda x : self.get_new_phrase(np.random.choice(data)), range(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2015bd-93b9-45ff-9fb8-bf9e9470a53e",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo de Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0978e2-5d0e-4e6d-b4f8-3f530e474c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20bb259-dab2-4d28-bcbe-e308f53a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    if \"data_found\" in file:\n",
    "        texts.append(\"./Data/\" + file)\n",
    "        \n",
    "texts.append(\"./Data/Spanish Toxicity Dataset.csv\")\n",
    "texts.append(\"./Data/public_test_data.csv\")\n",
    "texts.append(\"./Data/referenced_tweets_data.csv\")\n",
    "texts.append(\"./Data/tweets_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aabc4a24-f588-456d-831e-8ac4f70cc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = Genetic_model(params = \"./Parameters/parameters.json\", nlp_model = \"./Model/model\", population = 100, vector_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0d7958-3ca3-4436-ac55-77716ad04fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer.train_nlp_model(texts, True, \"./Model/model\")\n",
    "Trainer.load_word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "551b324e-0602-4e75-9d58-79c8a1a37a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weno', 0.717048168182373),\n",
       " ('chistosa', 0.7039027214050293),\n",
       " ('charcha', 0.6815004348754883),\n",
       " ('fomeee', 0.6788569688796997),\n",
       " ('pulento', 0.6730825304985046),\n",
       " ('ooooh', 0.6536873579025269),\n",
       " ('buta', 0.6492932438850403),\n",
       " ('penka', 0.6486002206802368),\n",
       " ('youngcis', 0.647746205329895),\n",
       " ('uta', 0.6453183889389038)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer.word2vec.wv.most_similar(\"wena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f60d0d-1cfe-4756-a957-61b77784667d",
   "metadata": {},
   "source": [
    "## Creamos la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ab80c-6f31-4865-9959-f823d6562756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c2b0c1-9d38-44e5-975d-17afacd4f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/tweets_train.csv\")\n",
    "referenced = pd.read_csv(\"./Data/referenced_tweets_data.csv\")[[\"tweet_id\", \"conversation_id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc58ebf-8a95-4b94-8b50-df752c088dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x : Trainer.preprocess_text(x))\n",
    "referenced[\"text\"] = referenced[\"text\"].apply(lambda x : Trainer.preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943ae523-6d2f-4d71-8358-189e93fb934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {}\n",
    "for l in referenced.values.tolist():\n",
    "    tweet_id = l[0]\n",
    "    conv_id = l[1]\n",
    "    text_string = l[2]\n",
    "    \n",
    "    if conv_id not in transform and conv_id != 0:\n",
    "        transform[conv_id] = []\n",
    "    if conv_id != 0:\n",
    "        transform[conv_id].append((tweet_id, text_string.strip(\" \")))\n",
    "        transform[conv_id].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9872a95-cb06-4779-8e27-94ceda7202b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"text\", \"Odio\", \"Mujeres\", \"Comunidad LGBTQ+\", \"Comunidades Migrantes\", \"Pueblos Originarios\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d299b635-9bd5-4ac0-99ef-a634ffa8c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Odio\"] = data[\"Odio\"].apply(lambda x : 1 - int(x == 0))\n",
    "data[\"Mujeres\"] = data[\"Mujeres\"].apply(lambda x : 1 - int(x == 0))\n",
    "data[\"Comunidad LGBTQ+\"] = data[\"Comunidad LGBTQ+\"].apply(lambda x : 1 - int(x == 0))\n",
    "data[\"Comunidades Migrantes\"] = data[\"Comunidades Migrantes\"].apply(lambda x : 1 - int(x == 0))\n",
    "data[\"Pueblos Originarios\"] = data[\"Pueblos Originarios\"].apply(lambda x : 1 - int(x == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd009ca-a68a-471e-9b79-a9eb092d4cb9",
   "metadata": {},
   "source": [
    "### Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaef0e39-c45f-4aee-9a9a-0f01bd0b9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_new_data = 1000\n",
    "\n",
    "hate_data = data.query(\"Odio == 1\")[\"text\"]\n",
    "not_hate_data = data.query(\"Odio == 0\")[\"text\"]\n",
    "\n",
    "new_hate_data = Trainer.create_new_data(hate_data.values.tolist(), number_of_new_data)\n",
    "new_nohate_data = Trainer.create_new_data(not_hate_data.values.tolist(), number_of_new_data)\n",
    "\n",
    "text_column = data[\"text\"].values.tolist() + new_hate_data + new_nohate_data\n",
    "hate_column = data[\"Odio\"].values.tolist() + [ 1 for i in range(len(new_hate_data))] + [ 1 for i in range(len(new_nohate_data))]\n",
    "\n",
    "full_data = pd.DataFrame({\"text\" : text_column, \"Odio\" : hate_column})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65713956-5c2e-4dff-980c-7b6ec3c07a4c",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29d7d945-d0de-4c11-a91e-baa9d351bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"text\"]], data[[\"Odio\", \"Mujeres\", \"Comunidad LGBTQ+\", \"Comunidades Migrantes\", \"Pueblos Originarios\"]], test_size = 0.1)\n",
    "y_train = y_train.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83f724ed-c423-41c2-9eff-fa44c3955324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.k_fold(X_train.copy(), y_train.copy(), 5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6109c6b-3626-43ba-a30e-af2eab7a2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.make_nns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67a8c252-c9b2-4939-9530-1ab83c5330d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(Trainer.neural_networks[0], to_file=\"my_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f49d75d6-b595-45c8-9015-ac443eef56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation : 0 - prob : 0.35, importance : 0.3\n",
      "Best score : 1\n",
      "Actual best : 0\n",
      "Training neural network : 2\n",
      "Epoch 1/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 2.0691 - categorical_crossentropy: 2.0691 - val_loss: 2.1475 - val_categorical_crossentropy: 2.1475\n",
      "Epoch 2/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 2.0509 - categorical_crossentropy: 2.0509 - val_loss: 2.1323 - val_categorical_crossentropy: 2.1323\n",
      "Epoch 3/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 2.0365 - categorical_crossentropy: 2.0365 - val_loss: 2.1187 - val_categorical_crossentropy: 2.1187\n",
      "Epoch 4/32\n",
      "115/115 [==============================] - 2s 21ms/step - loss: 2.0236 - categorical_crossentropy: 2.0236 - val_loss: 2.1065 - val_categorical_crossentropy: 2.1065\n",
      "Epoch 5/32\n",
      "115/115 [==============================] - 3s 22ms/step - loss: 2.0119 - categorical_crossentropy: 2.0119 - val_loss: 2.0959 - val_categorical_crossentropy: 2.0959\n",
      "Epoch 6/32\n",
      "115/115 [==============================] - 2s 21ms/step - loss: 2.0018 - categorical_crossentropy: 2.0018 - val_loss: 2.0869 - val_categorical_crossentropy: 2.0869\n",
      "Epoch 7/32\n",
      "115/115 [==============================] - 2s 22ms/step - loss: 1.9930 - categorical_crossentropy: 1.9930 - val_loss: 2.0791 - val_categorical_crossentropy: 2.0791\n",
      "Epoch 8/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9852 - categorical_crossentropy: 1.9852 - val_loss: 2.0722 - val_categorical_crossentropy: 2.0722\n",
      "Epoch 9/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9784 - categorical_crossentropy: 1.9784 - val_loss: 2.0659 - val_categorical_crossentropy: 2.0659\n",
      "Epoch 10/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9722 - categorical_crossentropy: 1.9722 - val_loss: 2.0603 - val_categorical_crossentropy: 2.0603\n",
      "Epoch 11/32\n",
      "115/115 [==============================] - 3s 22ms/step - loss: 1.9667 - categorical_crossentropy: 1.9667 - val_loss: 2.0553 - val_categorical_crossentropy: 2.0553\n",
      "Epoch 12/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9617 - categorical_crossentropy: 1.9617 - val_loss: 2.0509 - val_categorical_crossentropy: 2.0509\n",
      "Epoch 13/32\n",
      "115/115 [==============================] - 2s 21ms/step - loss: 1.9571 - categorical_crossentropy: 1.9571 - val_loss: 2.0468 - val_categorical_crossentropy: 2.0468\n",
      "Epoch 14/32\n",
      "115/115 [==============================] - 3s 22ms/step - loss: 1.9529 - categorical_crossentropy: 1.9529 - val_loss: 2.0432 - val_categorical_crossentropy: 2.0432\n",
      "Epoch 15/32\n",
      "115/115 [==============================] - 3s 23ms/step - loss: 1.9491 - categorical_crossentropy: 1.9491 - val_loss: 2.0399 - val_categorical_crossentropy: 2.0399\n",
      "Epoch 16/32\n",
      "115/115 [==============================] - 2s 21ms/step - loss: 1.9456 - categorical_crossentropy: 1.9456 - val_loss: 2.0369 - val_categorical_crossentropy: 2.0369\n",
      "Epoch 17/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9423 - categorical_crossentropy: 1.9423 - val_loss: 2.0341 - val_categorical_crossentropy: 2.0341\n",
      "Epoch 18/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9393 - categorical_crossentropy: 1.9393 - val_loss: 2.0315 - val_categorical_crossentropy: 2.0315\n",
      "Epoch 19/32\n",
      "115/115 [==============================] - 2s 20ms/step - loss: 1.9365 - categorical_crossentropy: 1.9365 - val_loss: 2.0292 - val_categorical_crossentropy: 2.0292\n",
      "Epoch 20/32\n",
      " 66/115 [================>.............] - ETA: 0s - loss: 1.9538 - categorical_crossentropy: 1.9538Epoch 1/32\n",
      " 16/115 [===>..........................] - ETA: 1s - loss: 2.1244 - categorical_crossentropy: 2.1244"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Trainer.train_hate(generations = 5, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4570ea5-b4f0-4ce1-9471-b5d511003c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.neural_networks[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b0726-790d-46ca-8915-dd3e8ebab860",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.set_best_hate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c9fe6-bb9f-4223-baa1-3b581afe0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.predict_hate(pd.DataFrame({\n",
    "    \"text\" : [\"\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883cd12-0a65-4eed-819e-a2e639a72c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc820b5a-8834-418d-a21a-214381814143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Trainer.neural_networks[0]\n",
    "Trainer.model_hate = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96760e1-c433-44ca-a643-56d28f3cace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"text\" : [\"yo no creeria en lo que dice\"]})\n",
    "Trainer.f1_macro(tf.cast([[1,0]], tf.float32), tf.cast(Trainer.predict_hate(test).round().tolist(), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de06777-f15f-40ec-b1d5-89b96e6ad2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genetic_model_v2(Genetic_model):\n",
    "    def __init__(self, nn_model = \"\", nlp_model = \"\", params = \"\", vector_size = 100,population = 50, min_count = 2):\n",
    "        self.nn_model = nn_model\n",
    "        self.nlp_model = nlp_model\n",
    "        self.parameters = params\n",
    "        self.vector_size = vector_size\n",
    "        self.population = population\n",
    "        self.min_count = min_count\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        self.metrics_binary = [self.f1_macro]\n",
    "        \n",
    "    def create_nn(self):\n",
    "        input_layer = tf.keras.Input(shape = self.vector_size)\n",
    "        layer_1 = tf.keras.layers.Dense(50, activation = \"relu\")(input_layer)\n",
    "        layer_2 = tf.keras.layers.Dense(25, activation = \"relu\")(layer_1)\n",
    "        output = tf.keras.layers.Dense(1, activation = \"sigmoid\")(layer_1)\n",
    "\n",
    "        model = tf.keras.Model(inputs = input_layer, outputs = output)\n",
    "        model.compile(\n",
    "            optimizer = self.optimizer,\n",
    "            loss = tf.keras.losses.BinaryCrossentropy() ,\n",
    "            metrics = self.metrics_binary\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4192c6-8a9b-4b56-a46a-d5abd2c8190f",
   "metadata": {},
   "source": [
    "# Reuse a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95a201-cf9d-4cca-a421-d52dc01443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape = [], dtype = tf.string, trainable = False, name = \"Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d0957-97eb-48bd-8b1a-a082f69a6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"text\"]], data[[\"Odio\", \"Mujeres\", \"Comunidad LGBTQ+\", \"Comunidades Migrantes\", \"Pueblos Originarios\"]], test_size = 0.1)\n",
    "y_train = y_train.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df3d58-3ba9-4f57-8f70-01544fce72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = tf.constant(X_train[\"text\"].values.tolist())\n",
    "y_train_tensor = tf.constant(y_train[[\"Odio\", \"Mujeres\", \"Comunidad LGBTQ+\", \"Comunidades Migrantes\", \"Pueblos Originarios\"]].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb380f-a700-465d-8932-8355c5828ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.Input(shape=(), dtype = tf.string, name = \"Input\")\n",
    "\n",
    "input_layer = hub_layer(text_input)\n",
    "\n",
    "layer_1 = tf.keras.layers.Dense(16, activation = \"relu\", name = \"Layer_1\", use_bias = True)(input_layer)\n",
    "layer_2 = tf.keras.layers.Dense(10, activation = \"relu\", name = \"Layer_2\", use_bias = True)(layer_1)\n",
    "out_1 = tf.keras.layers.Dense(1, activation = \"sigmoid\", name = \"Out_1\", use_bias = True)(layer_2)\n",
    "out_2 = tf.keras.layers.Dense(4, activation = \"sigmoid\", name = \"Out_2\", use_bias = True)(tf.concat([out_1, layer_2], 1))\n",
    "\n",
    "model = tf.keras.Model(inputs = text_input, outputs = tf.concat([out_1, out_2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3c913-13a3-4845-b177-59def4b69677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = tf.keras.losses.categorical_crossentropy,\n",
    "              metrics = tf.keras.metrics.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d1353-95f1-4194-bb65-72aaf55e79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(\"val_loss\", patience = 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc06886-bf7c-4079-8c36-9e763f08c288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_tensor, y_train_tensor, validation_split = 0.1, epochs = 100, callbacks = callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
